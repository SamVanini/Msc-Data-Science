{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8a7798f",
   "metadata": {},
   "source": [
    "# Data analysis from zero to hero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670b1cb4",
   "metadata": {},
   "source": [
    "## Imports section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cee22fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import sklearn\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b7e3fe",
   "metadata": {},
   "source": [
    "## First step: load your data set\n",
    "\n",
    "For analyze a bunch of data you have, of course, to load it in the right way.\n",
    "\n",
    "Depending on the type of data you are gonna analyze, there are various ways to import the related DataFrame.\n",
    "For instance:\n",
    "- CSV: pd.read_csv(\"file/path/to/data.csv\", sep=\";\")\n",
    "- Excel: pd.read_excel(\"file/path/to/data.xlsx\")\n",
    "- JSON: pd.read_json(\"file/path/to/data.json\")\n",
    "- HTML: pd.read_html(\"file/path/to/data.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3064d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(\"sample_data\", \"housing.csv\")\n",
    "df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a402f951",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "First of all, you have to look arond your data, in order to understand the basic features of the data frame\n",
    "\n",
    "Adviced steps are the following one:\n",
    "- Visualize the first 5 and the last 5 rows using df.head() and df.tail() respectively\n",
    "- Have a look at the shape of the data set, using df.shape\n",
    "- Visualize the available columns using df.columns\n",
    "- Check data types using df.dtypes\n",
    "\n",
    "For a brief look at the statistics and null values:\n",
    "- df.describe() and df.describe().T\n",
    "- df.info()\n",
    "- df.isna().sum().sum() for overall count, only one sum for count by column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca07578",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8d2163",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7754ab8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e87db75",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75388ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8219a56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71da33a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f7ed71",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b33c05",
   "metadata": {},
   "source": [
    "## Manage null values\n",
    "\n",
    "It can happen to have null values in columns\n",
    "\n",
    "Depending on their importance in your analysis you can either drop them or fill them with a suitable value (e.g the column mean)\n",
    "\n",
    "Note: this type of operation should be done on a copy of the original dataset, better safe than sorry\n",
    "\n",
    "Note 2: the use of inPlace=True on some operations has been marked with a FutureWarning, so it is better to avoid it in favour of assignment. Other reasons to do it are clarity, readibility, and intention driven behaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119461dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['column_with_null_values'] = df['column_with_null_values'].dropna()\n",
    "# df['column_with_null_values'] = df['column_with_null_values'].fillna(df['column_with_null_values'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201c3e62",
   "metadata": {},
   "source": [
    "## Filtering data\n",
    "\n",
    "You may want to visualize or treat only a slice of the original Pandas series (column).\n",
    "It can be done using boolean masks, their syntax is quite simple, but they are very useful sometimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1634b691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# boolean_mask = series_name > value\n",
    "# boolean_mask_and = (series_name > value) & (series_name < value)\n",
    "# boolean_mask_or = (series_name < value) || (series_name == value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b601cc5e",
   "metadata": {},
   "source": [
    "Another useful feature could be column renaming, in order to clarify what does that series contain. Or, more trivial, you may want to capitalize or format the name somehow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb2e589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.columns = df.columns.map(lamda col: col.capitalize())\n",
    "# df.columns = df.columns.map(str.capitalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6d62cb",
   "metadata": {},
   "source": [
    "## Plotting\n",
    "\n",
    "Visualize the data you are working with in order to extract hidden features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98e71ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist(figsize=(10, 10), bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acb4fde",
   "metadata": {},
   "source": [
    "### Simple plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10840928",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(range(0, 21))\n",
    "y = [i**2 for i in x]\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(x, y)\n",
    "plt.title('Square of X')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.xticks(x)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521c888a",
   "metadata": {},
   "source": [
    "### Scatter\n",
    "\n",
    "c indicates the color to give at the scatter chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e4ef98",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(x, y, c=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7d3c1f",
   "metadata": {},
   "source": [
    "### Bar Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbd1c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['MS', 'Apple', 'Meta', 'Google', 'Amazon']\n",
    "stock_prices = [130, 112, 145, 180, 201]\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "bars = plt.bar(labels, stock_prices)\n",
    "bars[2].set_hatch('.')\n",
    "bars[0].set_width(0.2)\n",
    "bars[1].set_color('red')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9122ef3e",
   "metadata": {},
   "source": [
    "### Heat map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32829c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "sb.heatmap(df.corr(), annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc25157",
   "metadata": {},
   "source": [
    "## Data Wrangling\n",
    "\n",
    "This step is needed to remove outliers in order to have a smoother data set\n",
    "\n",
    "The procedure here usually consists in replacing the outliers with the mean value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446951d0",
   "metadata": {},
   "source": [
    "## Aggregations and grouping\n",
    "\n",
    "Like an SQL statement, it may be necessary to group data by a column in order to analyze data by categories or groups, such as calculating summary statistics like the mean, sum, or count for each group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2350d148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouped_df = df.groupby('COL_NAME').agg({'COL_1': fun1, 'COL_2': fun2})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5095bd79",
   "metadata": {},
   "source": [
    "### Concat, Merge and Join\n",
    "\n",
    "These are three methods for combining DataFrames, each with different use cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0e9a01",
   "metadata": {},
   "source": [
    "#### Concat\n",
    "\n",
    "Concatenates DataFrames along an axis (row-wise or column-wise).\n",
    "\n",
    "**Use case**: When you have similar structured data you want to append (like monthly reports into yearly data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74d05a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'A': [1, 2], 'B': [3, 4]})\n",
    "df2 = pd.DataFrame({'A': [5, 6], 'B': [7, 8]})\n",
    "\n",
    "# Stack vertically (add rows)\n",
    "result = pd.concat([df1, df2], axis=0)\n",
    "\n",
    "# Stack horizontally (add columns)\n",
    "result = pd.concat([df1, df2], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24860d27",
   "metadata": {},
   "source": [
    "#### Merge\n",
    "\n",
    "Concatenates DataFrames based on common columns/keys (like SQL JOIN).\n",
    "\n",
    "**Warning**: If both key columns contain rows where the key is a null value, those rows will be matched against each other. This is different from usual SQL join behaviour and can lead to unexpected results.\n",
    "\n",
    "**Use case**: When you need to combine data based on matching values (like joining customer info with orders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aec910c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'key': ['A', 'B'], 'value1': [1, 2]})\n",
    "df2 = pd.DataFrame({'key': ['A', 'C'], 'value1': [3, 4]})\n",
    "\n",
    "# Inner join (default)\n",
    "# Left/right/outer/cross joins are also available\n",
    "result = pd.merge(df1, df2, on='key', how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd94a09d",
   "metadata": {},
   "source": [
    "#### Join\n",
    "\n",
    "Like merge, but joins on indices by default (convenient shortcut)\n",
    "\n",
    "**Use case**: When DataFrames have meaningful indices you want to join on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f09a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'A': [1, 2]}, index=['x', 'y'])\n",
    "df2 = pd.DataFrame({'B': [3, 4]}, index=['x', 'z'])\n",
    "\n",
    "# Join on index (left join by default)\n",
    "result = df1.join(df2, how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3174395a",
   "metadata": {},
   "source": [
    "## Learning models\n",
    "\n",
    "When using sklearn in production/data science projects, follow this standard workflow:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9b4133",
   "metadata": {},
   "source": [
    "### 1. Data Preparation and Exploration\n",
    "\n",
    "Follow the steps shown in the previous sections of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0226b1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and explore data\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data.csv')\n",
    "\n",
    "# EDA: check shape, info, describe, missing values, distributions\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bb6fb3",
   "metadata": {},
   "source": [
    "### 2. Train-Test Split (Always First!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec8a666",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split BEFORE any preprocessing to prevent data leakage\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140840ad",
   "metadata": {},
   "source": [
    "### 3. Preprocessing Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2731dd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Fit on training data only, transform both \n",
    "scaler = StandardScaler() \n",
    "X_train_scaled = scaler.fit_transform(X_train) \n",
    "X_test_scaled = scaler.transform(X_test) # Don't fit on test! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881e4f13",
   "metadata": {},
   "source": [
    "### 4. Model Training and Cross-Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55bbd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "model = LogisticRegression() # Validate on training set using CV \n",
    "cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5) \n",
    "print(f\"CV Score: {cv_scores.mean():.3f} (+/- {cv_scores.std():.3f})\") \n",
    "\n",
    "# Train final model \n",
    "model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d91e54",
   "metadata": {},
   "source": [
    "### 5. Hyperparameter Tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb0b469",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'C': [0.1, 1, 10], 'penalty': ['l1', 'l2']} \n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy') \n",
    "grid_search.fit(X_train_scaled, y_train) \n",
    "best_model = grid_search.best_estimator_ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19882d8e",
   "metadata": {},
   "source": [
    "### 6. Evaluation on Test Set (Only Once!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e03b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "y_pred = best_model.predict(X_test_scaled)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c75a83",
   "metadata": {},
   "source": [
    "### 7. Use Pipelines for Production "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d208236",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Complete pipeline (preprocessing + model) \n",
    "pipeline = Pipeline([('scaler', StandardScaler()), ('model', LogisticRegression()) ])\n",
    "pipeline.fit(X_train, y_train) # Pipeline handles everything"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929ddc1f",
   "metadata": {},
   "source": [
    "### Key principles\n",
    "- Split data first, then preprocess\n",
    "- Fit preprocessing only on training data\n",
    "- Use cross-validation on training set for model selection\n",
    "- Test set touched only once for final evaluation\n",
    "- Use Pipelines to ensure consistent transformations\n",
    "- Set random_state for reproducibility\n",
    "- Never fit scaler/encoder on entire dataset before splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5cece7",
   "metadata": {},
   "source": [
    "### Main sklearn modules overview\n",
    "\n",
    "#### 1. preprocessing \n",
    "- Purpose: Transform and normalize data before training \n",
    "- Key functions: StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder - Use cases:   \n",
    "    - Scaling features to similar ranges (0-1 or mean=0, std=1)   \n",
    "    - Encoding categorical variables   \n",
    "    - Handling missing values   \n",
    "    - Feature normalization for algorithms sensitive to feature scales (SVM, neural networks)\n",
    " \n",
    "#### 2. linear_model \n",
    "- Purpose: Linear regression and classification models\n",
    "- Key models: LinearRegression, LogisticRegression, Ridge, Lasso, ElasticNet \n",
    "- Use cases:\n",
    "    - Predicting continuous values (house prices, sales)\n",
    "    - Binary/multi-class classification\n",
    "    - Feature selection with regularization\n",
    "    - When interpretability is important\n",
    "\n",
    "#### 3. naive_bayes \n",
    "- Purpose: Probabilistic classification based on Bayes' theorem \n",
    "- Key models: GaussianNB, MultinomialNB, BernoulliNB \n",
    "- Use cases:   \n",
    "    - Text classification (spam detection, sentiment analysis)\n",
    "    - Document categorization\n",
    "    - Real-time prediction (fast training)\n",
    "    - When features are independent\n",
    " \n",
    "#### 4. model_selection\n",
    "- Purpose: Model evaluation and hyperparameter tuning \n",
    "- Key functions: train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV \n",
    "- Use cases:\n",
    "    - Splitting data into train/test sets\n",
    "    - Cross-validation to prevent overfitting\n",
    "    - Finding optimal hyperparameters\n",
    "    - Model performance comparison\n",
    " \n",
    "#### 5. metrics\n",
    "- Purpose: Evaluate model performance\n",
    "- Key functions: accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score, mean_squared_error\n",
    "- Use cases:\n",
    "    - Classification metrics (accuracy, precision, recall)\n",
    "    - Regression metrics (MSE, MAE, R²)\n",
    "    - ROC curves for threshold selection\n",
    "    - Model comparison\n",
    " \n",
    "#### 6. ensemble \n",
    "- Purpose: Combine multiple models for better performance \n",
    "- Key models: RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, VotingClassifier \n",
    "- Use cases:\n",
    "    - Reducing overfitting (Random Forest)\n",
    "    - Improving prediction accuracy\n",
    "    - Handling complex non-linear relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ce2825",
   "metadata": {},
   "source": [
    "### RandomForestClassifier overview\n",
    "\n",
    "The typical procedure for using RandomForestClassifier from scikit-learn:\n",
    "\n",
    "1. Import the classifier:\n",
    "2. Prepare your data:\n",
    "3. Create and configure the model:\n",
    "4. Train the model:\n",
    "5. Make predictions:\n",
    "6. Evaluate the model:\n",
    "\n",
    "**Key parameters to tune:**\n",
    "- `n_estimators:` Number of trees in the forest (default: 100)\n",
    "- `max_depth:` Maximum depth of each tree\n",
    "- `min_samples_split:` Minimum samples required to split a node\n",
    "- `max_features:` Number of features to consider for best split\n",
    "- `class_weight:` Handle imbalanced datasets\n",
    "\n",
    "#### Tips and tricks for selecting features and target\n",
    "\n",
    "**Target Variable (y)**\n",
    "- Must be the column you want to predict\n",
    "- Should be categorical for classification (e.g., 'species', 'diagnosis', 'category')\n",
    "- Should be a single column\n",
    "\n",
    "**Features (X)**\n",
    "\n",
    "These are the input variables used to predict y.\n",
    "\n",
    "Selection criteria:\n",
    "1. Exclude the target variable\n",
    "2. Remove irrelevant columns:\n",
    "    - ID columns (e.g., 'customer_id', 'transaction_id')\n",
    "    - Timestamps (unless using as features after processing)\n",
    "    - Columns with constant values\n",
    "3. Consider correlation:\n",
    "    - Include features that correlate with the target\n",
    "    - Remove highly correlated features (multicollinearity)\n",
    "4. Handle data types:\n",
    "    - Numerical features can be used directly\n",
    "    - Categorical features need encoding (one-hot, label encoding)\n",
    "\n",
    "__Best practice:__ Start with domain knowledge to identify relevant features, then refine using feature importance or selection techniques."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
